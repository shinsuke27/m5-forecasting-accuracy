{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "pd.set_option('display.max_columns', 500)\n",
    "mlflow.set_tracking_uri('http://mlflow:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5:\n",
    "    def __init__(self, START_TRAIN):\n",
    "        self.TARGET = 'sales'\n",
    "        self.START_TRAIN = START_TRAIN\n",
    "        self.END_TRAIN = 1913\n",
    "        self.START_VALID = 1914\n",
    "        self.END_VALID = 1941\n",
    "        self.START_TEST = 1942\n",
    "        self.END_TEST = 1969\n",
    "        self.P_HORIZON = 28\n",
    "        self.SEED = 2020\n",
    "        self.N_CORES = psutil.cpu_count()\n",
    "        self.INPUT = '/kaggle/input'\n",
    "        self.ORIGINAL = f'{self.INPUT}/m5-forecasting-accuracy'\n",
    "        self.BASE = f'{self.INPUT}/m5-simple-fe/grid_part_1.pkl'\n",
    "        self.PRICE = f'{self.INPUT}/m5-simple-fe/grid_part_2.pkl'\n",
    "        self.CALENDAR = f'{self.INPUT}/m5-simple-fe/grid_part_3.pkl'\n",
    "        self.LAGS = f'{self.INPUT}/m5-lags-features/lags_df_28.pkl'\n",
    "        self.MEAN_ENC = f'{self.INPUT}/m5-custom-features/mean_encoding_df.pkl'\n",
    "        self.STORES_IDS = list(pd.read_csv(f'{self.ORIGINAL}/sales_train_evaluation.csv')['store_id'].unique())\n",
    "        self.SHIFT_DAY = 28\n",
    "        self.N_LAGS = 15\n",
    "        self.LAGS_SPLIT = [col for col in range(1, self.SHIFT_DAY + self.N_LAGS)]\n",
    "        self.ROLS_SPLIT = [[i, j] for i in [1, 7, 14] for j in [7, 14, 30, 60]]\n",
    "        self.mean_features = [\n",
    "            'enc_cat_id_mean',\n",
    "            'enc_cat_id_std',\n",
    "            'enc_dept_id_mean',\n",
    "            'enc_dept_id_std',\n",
    "            'enc_item_id_mean',\n",
    "            'enc_item_id_std',\n",
    "        ]\n",
    "        self.feature_columns = [\n",
    "            'item_id',\n",
    "            'dept_id',\n",
    "            'cat_id',\n",
    "            'release',\n",
    "            'sell_price',\n",
    "            'price_max',\n",
    "            'price_min',\n",
    "            'price_std',\n",
    "            'price_mean',\n",
    "            'price_norm',\n",
    "            'price_nunique',\n",
    "            'item_nunique',\n",
    "            'price_momentum',\n",
    "            'price_momentum_m',\n",
    "            'price_momentum_y',\n",
    "            'event_name_1',\n",
    "            'event_type_1',\n",
    "            'event_name_2',\n",
    "            'event_type_2',\n",
    "            'snap_CA',\n",
    "            'snap_TX',\n",
    "            'snap_WI',\n",
    "            'tm_d',\n",
    "            'tm_w',\n",
    "            'tm_m',\n",
    "            'tm_y',\n",
    "            'tm_wm',\n",
    "            'tm_dw',\n",
    "            'tm_w_end',\n",
    "            'enc_cat_id_mean',\n",
    "            'enc_cat_id_std',\n",
    "            'enc_dept_id_mean',\n",
    "            'enc_dept_id_std',\n",
    "            'enc_item_id_mean',\n",
    "            'enc_item_id_std',\n",
    "        ]\n",
    "        self.lag_columns = [col for col in pd.read_pickle(self.LAGS).columns if 'lag_' in col or 'rolling_' in col]\n",
    "\n",
    "    def seed_everything(self):\n",
    "        seed = self.SEED\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def get_data_by_store(self, store_id):\n",
    "        df = pd.read_pickle(self.BASE).query(f'd >= {self.START_TRAIN}').reset_index(drop=True)\n",
    "        \n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.PRICE),\n",
    "            on=['id', 'd']\n",
    "        ).merge(\n",
    "            pd.read_pickle(self.CALENDAR),\n",
    "            on=['id', 'd']\n",
    "        ).query(f'store_id == \"{store_id}\"')\n",
    "        \n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.MEAN_ENC)[['id', 'd'] + self.mean_features],\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "        \n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.LAGS).drop('sales', axis=1),\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "        \n",
    "        # Christmasの日を落とす\n",
    "        df = df.query('event_name_1 != \"Christmas\"').reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def get_data(self):\n",
    "        df = pd.read_pickle(self.BASE).query(f'd >= {self.START_TRAIN}').reset_index(drop=True)\n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.PRICE),\n",
    "            on=['id', 'd']\n",
    "        ).merge(\n",
    "            pd.read_pickle(self.CALENDAR),\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "\n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.MEAN_ENC)[['id', 'd'] + self.mean_features],\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "\n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.LAGS).drop('sales', axis=1),\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "\n",
    "        # Christmasの日を落とす\n",
    "        df = df.query('event_name_1 != \"Christmas\"').reset_index(drop=True)\n",
    "\n",
    "        df = df.sort_values(['d', 'id']).reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "    def train_test_split(self, df):\n",
    "        df = df.sort_values(['d', 'id'])\n",
    "        train_df = df.query(f'd <= {self.END_VALID}').reset_index(drop=True)\n",
    "        test_df = df.query(f'd > {self.END_VALID}').reset_index(drop=True)\n",
    "        return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    def __init__(self, train_df, test_df, features, target, seed, n_splits=3, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.verbose = verbose\n",
    "        self.target = target\n",
    "        self.seed = seed\n",
    "        self.params = self.get_params()\n",
    "        self.y_pred, self.score, self.model = self.fit()\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "\n",
    "    def generate_importance_fig(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self):\n",
    "        y_pred = np.zeros((len(self.test_df), ))\n",
    "\n",
    "        mlflow.log_param('train_period', self.train_df['d'].max() - self.train_df['d'].min())\n",
    "        mlflow.log_params(self.params)\n",
    "        models = []\n",
    "        \n",
    "        oof_pred = np.array([])\n",
    "        oof_true = np.array([])\n",
    "        for fold in range(1, 1+self.n_splits):\n",
    "            val_idx = self.train_df.query(f'd >= {1942 - 28*fold} and d <= {1941 - 28*(fold-1)}').index\n",
    "            train_idx = self.train_df.query(f'd < {1942 - 28*fold}').index\n",
    "\n",
    "            print(f'FOLD:', fold)\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            models.append(model)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            fold_pred = model.predict(conv_x_val).reshape(self.train_df[self.target][val_idx].shape)\n",
    "            oof_pred = np.concatenate((oof_pred, fold_pred), axis=None)\n",
    "            oof_true = np.concatenate((oof_true, y_val), axis=None)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            fold_loss_score = np.sqrt(mean_squared_error(y_val, oof_pred[val_idx]))\n",
    "            print(f'Partial score of fold {fold} is:', fold_loss_score)\n",
    "            del x_train, x_val, y_train, y_val, train_set, val_set, model, conv_x_val, x_test; gc.collect()\n",
    "\n",
    "        loss_score = np.sqrt(mean_squared_error(oof_true, oof_pred))\n",
    "        print('Our oof rmse score is: ', loss_score)\n",
    "        mlflow.log_metric('rmse', loss_score)\n",
    "        self.generate_importance_fig(models, 'importance')\n",
    "        return y_pred, loss_score, models\n",
    "\n",
    "class Lgb_Model(Base_Model):\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(\n",
    "            self.params,\n",
    "            train_set,\n",
    "            num_boost_round=10000,\n",
    "            early_stopping_rounds=100,\n",
    "            valid_sets=[train_set, val_set],\n",
    "            verbose_eval=verbosity,\n",
    "        )\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "\n",
    "    def get_params(self):        \n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'tweedie',\n",
    "            'tweedie_variance_power': 1.1,\n",
    "            'metric': 'rmse',\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.005,\n",
    "            'num_leaves': 2**11-1,\n",
    "            'min_data_in_leaf': 2**12-1,\n",
    "            'feature_fraction': 0.5,\n",
    "            'max_bin': 100,\n",
    "            'boost_from_average': False,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "\n",
    "        params['seed'] = self.seed\n",
    "        return params\n",
    "\n",
    "    def generate_importance_fig(self, models, fig_path):\n",
    "        plt.figure(figsize=(12, 30))\n",
    "        plt.tight_layout()\n",
    "        importance = pd.DataFrame()\n",
    "        importance['column'] = models[0].feature_name()\n",
    "        importance['importance'] = sum([m.feature_importance('gain') for m in models])\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        sns.barplot(x='importance', y='column', data=importance)\n",
    "        plt.savefig(f'{fig_path}_gain.png', bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(f'{fig_path}_gain.png')\n",
    "\n",
    "        plt.figure(figsize=(12, 30))\n",
    "        plt.tight_layout()\n",
    "        importance = pd.DataFrame()\n",
    "        importance['column'] = models[0].feature_name()\n",
    "        importance['importance'] = sum([m.feature_importance('split') for m in models])\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        sns.barplot(x='importance', y='column', data=importance)\n",
    "        plt.savefig(f'{fig_path}_split.png', bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(f'{fig_path}_split.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a0cae2e398c45da8d29c0f72ca6324d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=28.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "day 1942\n",
      "Start Train: 1\n",
      "remove_features rolling_mean_180\n",
      "remove_features rolling_std_180\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4910c246c74fb985f06085d6fc04c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=165.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "drop NaN row... sales_lag_1 (46726819, 202)\n",
      "drop NaN row... sales_lag_2 (46696377, 202)\n",
      "drop NaN row... sales_lag_3 (46665887, 202)\n",
      "drop NaN row... sales_lag_4 (46635437, 202)\n",
      "drop NaN row... sales_lag_5 (46604998, 202)\n",
      "drop NaN row... sales_lag_6 (46574546, 202)\n",
      "drop NaN row... sales_lag_7 (46544056, 202)\n",
      "drop NaN row... sales_lag_8 (46513566, 202)\n",
      "drop NaN row... sales_lag_9 (46483107, 202)\n",
      "drop NaN row... sales_lag_10 (46452617, 202)\n",
      "drop NaN row... sales_lag_11 (46422177, 202)\n",
      "drop NaN row... sales_lag_12 (46391743, 202)\n",
      "drop NaN row... sales_lag_13 (46361281, 202)\n",
      "drop NaN row... sales_lag_14 (46330791, 202)\n",
      "drop NaN row... sales_lag_15 (46300301, 202)\n",
      "drop NaN row... sales_lag_16 (46269837, 202)\n",
      "drop NaN row... sales_lag_17 (46239347, 202)\n",
      "drop NaN row... sales_lag_18 (46208930, 202)\n",
      "drop NaN row... sales_lag_19 (46178472, 202)\n",
      "drop NaN row... sales_lag_20 (46148014, 202)\n",
      "drop NaN row... sales_lag_21 (46117527, 202)\n",
      "drop NaN row... sales_lag_22 (46087037, 202)\n",
      "drop NaN row... sales_lag_23 (46056571, 202)\n",
      "drop NaN row... sales_lag_24 (46026081, 202)\n",
      "drop NaN row... sales_lag_25 (45995648, 202)\n",
      "drop NaN row... sales_lag_26 (45965187, 202)\n",
      "drop NaN row... sales_lag_27 (45934729, 202)\n",
      "drop NaN row... sales_lag_28 (45904240, 202)\n",
      "drop NaN row... sales_lag_29 (45873750, 202)\n",
      "drop NaN row... sales_lag_30 (45843303, 202)\n",
      "drop NaN row... sales_lag_31 (45812813, 202)\n",
      "drop NaN row... sales_lag_32 (45782355, 202)\n",
      "drop NaN row... sales_lag_33 (45751892, 202)\n",
      "drop NaN row... sales_lag_34 (45721412, 202)\n",
      "drop NaN row... sales_lag_35 (45690922, 202)\n",
      "drop NaN row... sales_lag_36 (45660432, 202)\n",
      "drop NaN row... sales_lag_37 (45629964, 202)\n",
      "drop NaN row... sales_lag_38 (45599474, 202)\n",
      "drop NaN row... sales_lag_39 (45569038, 202)\n",
      "drop NaN row... sales_lag_40 (45538567, 202)\n",
      "drop NaN row... sales_lag_41 (45508091, 202)\n",
      "drop NaN row... sales_lag_42 (45477602, 202)\n",
      "drop NaN row... rolling_mean_7 (45477602, 202)\n",
      "drop NaN row... rolling_std_7 (45477602, 202)\n",
      "drop NaN row... rolling_mean_14 (45477602, 202)\n",
      "drop NaN row... rolling_std_14 (45477602, 202)\n",
      "drop NaN row... rolling_mean_30 (45020556, 202)\n",
      "drop NaN row... rolling_std_30 (45020556, 202)\n",
      "drop NaN row... rolling_mean_60 (44106936, 202)\n",
      "\n",
      "['item_id', 'dept_id', 'cat_id', 'release', 'sell_price', 'price_max', 'price_min', 'price_std', 'price_mean', 'price_norm', 'price_nunique', 'item_nunique', 'price_momentum', 'price_momentum_m', 'price_momentum_y', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'tm_d', 'tm_w', 'tm_m', 'tm_y', 'tm_wm', 'tm_dw', 'tm_w_end', 'enc_cat_id_mean', 'enc_cat_id_std', 'enc_dept_id_mean', 'enc_dept_id_std', 'enc_item_id_mean', 'enc_item_id_std', 'sales_lag_1', 'sales_lag_2', 'sales_lag_3', 'sales_lag_4', 'sales_lag_5', 'sales_lag_6', 'sales_lag_7', 'sales_lag_8', 'sales_lag_9', 'sales_lag_10', 'sales_lag_11', 'sales_lag_12', 'sales_lag_13', 'sales_lag_14', 'sales_lag_15', 'sales_lag_16', 'sales_lag_17', 'sales_lag_18', 'sales_lag_19', 'sales_lag_20', 'sales_lag_21', 'sales_lag_22', 'sales_lag_23', 'sales_lag_24', 'sales_lag_25', 'sales_lag_26', 'sales_lag_27', 'sales_lag_28', 'sales_lag_29', 'sales_lag_30', 'sales_lag_31', 'sales_lag_32', 'sales_lag_33', 'sales_lag_34', 'sales_lag_35', 'sales_lag_36', 'sales_lag_37', 'sales_lag_38', 'sales_lag_39', 'sales_lag_40', 'sales_lag_41', 'sales_lag_42', 'rolling_mean_7', 'rolling_std_7', 'rolling_mean_14', 'rolling_std_14', 'rolling_mean_30', 'rolling_std_30', 'rolling_mean_60', 'rolling_std_60', 'rolling_mean_tmp_1_7', 'rolling_mean_tmp_1_14', 'rolling_mean_tmp_1_30', 'rolling_mean_tmp_1_60', 'rolling_mean_tmp_2_7', 'rolling_mean_tmp_2_14', 'rolling_mean_tmp_2_30', 'rolling_mean_tmp_2_60', 'rolling_mean_tmp_3_7', 'rolling_mean_tmp_3_14', 'rolling_mean_tmp_3_30', 'rolling_mean_tmp_3_60', 'rolling_mean_tmp_4_7', 'rolling_mean_tmp_4_14', 'rolling_mean_tmp_4_30', 'rolling_mean_tmp_4_60', 'rolling_mean_tmp_5_7', 'rolling_mean_tmp_5_14', 'rolling_mean_tmp_5_30', 'rolling_mean_tmp_5_60', 'rolling_mean_tmp_6_7', 'rolling_mean_tmp_6_14', 'rolling_mean_tmp_6_30', 'rolling_mean_tmp_6_60', 'rolling_mean_tmp_7_7', 'rolling_mean_tmp_7_14', 'rolling_mean_tmp_7_30', 'rolling_mean_tmp_7_60', 'rolling_mean_tmp_8_7', 'rolling_mean_tmp_8_14', 'rolling_mean_tmp_8_30', 'rolling_mean_tmp_8_60', 'rolling_mean_tmp_9_7', 'rolling_mean_tmp_9_14', 'rolling_mean_tmp_9_30', 'rolling_mean_tmp_9_60', 'rolling_mean_tmp_10_7', 'rolling_mean_tmp_10_14', 'rolling_mean_tmp_10_30', 'rolling_mean_tmp_10_60', 'rolling_mean_tmp_11_7', 'rolling_mean_tmp_11_14', 'rolling_mean_tmp_11_30', 'rolling_mean_tmp_11_60', 'rolling_mean_tmp_12_7', 'rolling_mean_tmp_12_14', 'rolling_mean_tmp_12_30', 'rolling_mean_tmp_12_60', 'rolling_mean_tmp_13_7', 'rolling_mean_tmp_13_14', 'rolling_mean_tmp_13_30', 'rolling_mean_tmp_13_60', 'rolling_mean_tmp_14_7', 'rolling_mean_tmp_14_14', 'rolling_mean_tmp_14_30', 'rolling_mean_tmp_14_60', 'rolling_mean_tmp_15_7', 'rolling_mean_tmp_15_14', 'rolling_mean_tmp_15_30', 'rolling_mean_tmp_15_60', 'rolling_mean_tmp_16_7', 'rolling_mean_tmp_16_14', 'rolling_mean_tmp_16_30', 'rolling_mean_tmp_16_60', 'rolling_mean_tmp_17_7', 'rolling_mean_tmp_17_14', 'rolling_mean_tmp_17_30', 'rolling_mean_tmp_17_60', 'rolling_mean_tmp_18_7', 'rolling_mean_tmp_18_14', 'rolling_mean_tmp_18_30', 'rolling_mean_tmp_18_60', 'rolling_mean_tmp_19_7', 'rolling_mean_tmp_19_14', 'rolling_mean_tmp_19_30', 'rolling_mean_tmp_19_60', 'rolling_mean_tmp_20_7', 'rolling_mean_tmp_20_14', 'rolling_mean_tmp_20_30', 'rolling_mean_tmp_20_60', 'rolling_mean_tmp_21_7', 'rolling_mean_tmp_21_14', 'rolling_mean_tmp_21_30', 'rolling_mean_tmp_21_60', 'rolling_mean_tmp_22_7', 'rolling_mean_tmp_22_14', 'rolling_mean_tmp_22_30', 'rolling_mean_tmp_22_60', 'rolling_mean_tmp_23_7', 'rolling_mean_tmp_23_14', 'rolling_mean_tmp_23_30', 'rolling_mean_tmp_23_60', 'rolling_mean_tmp_24_7', 'rolling_mean_tmp_24_14', 'rolling_mean_tmp_24_30', 'rolling_mean_tmp_24_60', 'rolling_mean_tmp_25_7', 'rolling_mean_tmp_25_14', 'rolling_mean_tmp_25_30', 'rolling_mean_tmp_25_60', 'rolling_mean_tmp_26_7', 'rolling_mean_tmp_26_14', 'rolling_mean_tmp_26_30', 'rolling_mean_tmp_26_60', 'rolling_mean_tmp_27_7', 'rolling_mean_tmp_27_14', 'rolling_mean_tmp_27_30', 'rolling_mean_tmp_27_60', 'rolling_mean_tmp_28_7', 'rolling_mean_tmp_28_14', 'rolling_mean_tmp_28_30', 'rolling_mean_tmp_28_60']\n",
      "FOLD: 1\n",
      "Training until validation scores don't improve for 100 rounds\n"
     ]
    }
   ],
   "source": [
    "def convert_submit_data(row):\n",
    "    if row['d'] >= m5.START_VALID and row['d'] <= m5.END_VALID:\n",
    "        row['id'] = row['id'].replace('evaluation', 'validation')\n",
    "        row['d'] = f\"F{row['d'] - m5.END_TRAIN}\"\n",
    "    elif row['d'] >= m5.START_TEST and row['d'] <= m5.END_TEST:\n",
    "        row['id'] = row['id'].replace('validation', 'evaluation')\n",
    "        row['d'] = f\"F{row['d'] - m5.END_VALID}\"\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return row\n",
    "\n",
    "mlflow.set_experiment('RMSE')\n",
    "\n",
    "START_TRAIN = 1\n",
    "m5 = M5(START_TRAIN=START_TRAIN)\n",
    "\n",
    "pred_df = pd.DataFrame()\n",
    "\n",
    "df = m5.get_data()\n",
    "current_train_df, test_df = m5.train_test_split(df=df)\n",
    "\n",
    "for day in tqdm(range(m5.START_TEST, m5.END_TEST+1)):\n",
    "    print('day', day)\n",
    "    \n",
    "    submission = pd.DataFrame(columns=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv').columns)\n",
    "    print('Start Train:', START_TRAIN)\n",
    "    current_test_df = test_df.query(f'd == {day}').reset_index(drop=True)\n",
    "    remove_features = []\n",
    "\n",
    "    for col in m5.lag_columns:\n",
    "        if current_test_df[col].isnull().sum() > 0:\n",
    "            remove_features.append(col)\n",
    "            print('remove_features', col)\n",
    "    features = m5.feature_columns + [col for col in m5.lag_columns if col not in remove_features]\n",
    "\n",
    "    current_train_df = current_train_df.drop(remove_features, axis=1)\n",
    "    current_test_df = current_test_df.drop(remove_features, axis=1)\n",
    "\n",
    "#     trainで欠損値がある行を落とす\n",
    "    for c in tqdm(m5.lag_columns + ['sell_price']):\n",
    "        if c in features and current_train_df[col].isnull().sum() > 0:\n",
    "            current_train_df = current_train_df[current_train_df[c].notnull()].reset_index(drop=True)\n",
    "            print('drop NaN row...', c, current_train_df.shape)\n",
    "\n",
    "    print(features)\n",
    "    m5.seed_everything()\n",
    "\n",
    "    mlflow.start_run()\n",
    "    mlflow.log_param('START_TRAIN', START_TRAIN)\n",
    "    mlflow.log_param('PRED_DAY', day)\n",
    "\n",
    "    lgb_model = Lgb_Model(\n",
    "        train_df=current_train_df,\n",
    "        test_df=current_test_df,\n",
    "        features=features,\n",
    "        target=m5.TARGET,\n",
    "        seed=m5.SEED,\n",
    "    )\n",
    "\n",
    "    pred_df['id'] = current_test_df['id'].drop_duplicates()\n",
    "    pred_df[f'd_{day}'] = lgb_model.y_pred\n",
    "#     current_test_df = current_test_df[['id', 'd', 'pred']].apply(convert_submit_data, axis=1)\n",
    "#     submission = submission.append(pd.pivot(current_test_df, columns='d', index='id', values='pred').reset_index())\n",
    "#     submission.to_csv('./submission.csv', index=False)\n",
    "    pred_df.to_csv('./pred_df.csv', index=False)\n",
    "\n",
    "    mlflow.log_artifact('./pred_df.csv')\n",
    "    mlflow.end_run()\n",
    "    del m5, df, current_test_df, lgb_model; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
