{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "import os, sys, gc, time, warnings, pickle, psutil, random\n",
    "from multiprocessing import Pool\n",
    "from sklearn.model_selection import GroupKFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import mlflow\n",
    "import mlflow.lightgbm\n",
    "pd.set_option('display.max_columns', 500)\n",
    "mlflow.set_tracking_uri('http://mlflow:5000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv').drop([f'd_{i}' for i in range(1, 1886)], axis=1)\n",
    "# sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "# calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n",
    "\n",
    "# sales = sales.sort_values('id').reset_index(drop=True)\n",
    "# df = pd.melt(sales, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'd', value_name = 'sales')\n",
    "# df = df.sort_values(['d', 'id'])\n",
    "# df = df.merge(calendar[['d', 'wm_yr_wk']], on=['d']).merge(sell_prices, on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "# df = df.drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'wm_yr_wk'], axis=1)\n",
    "# df['d'] = df['d'].apply(lambda x: x.split('_')[-1]).astype(int)\n",
    "# df['total_sales'] = df['sales'] * df['sell_price']\n",
    "# df_valid = df[df['d'] <= 1913].reset_index(drop=True)\n",
    "# df_eval = df[df['d'] > 1913].reset_index(drop=True)\n",
    "# del df, calendar, sell_prices; gc.collect()\n",
    "\n",
    "# total_sales_valid = df_valid.groupby('id').sum()['total_sales'].values\n",
    "# total_sales_eval = df_eval.groupby('id').sum()['total_sales'].values\n",
    "\n",
    "# NUM_ITEMS = len(sales)\n",
    "# WEIGHTS_MAT_BOOL = np.c_[\n",
    "#     np.ones([NUM_ITEMS, 1]).astype(int), # Level 1\n",
    "#     pd.get_dummies(sales.state_id.astype(str), drop_first=False).astype(int).values, # Level 2\n",
    "#     pd.get_dummies(sales.store_id.astype(str), drop_first=False).astype(int).values, # Level 3\n",
    "#     pd.get_dummies(sales.cat_id.astype(str), drop_first=False).astype(int).values, # Level 4\n",
    "#     pd.get_dummies(sales.dept_id.astype(str), drop_first=False).astype(int).values, # Level 5\n",
    "#     pd.get_dummies(sales.state_id.astype(str) + sales.cat_id.astype(str), drop_first=False).astype(int).values, # Level 6\n",
    "#     pd.get_dummies(sales.state_id.astype(str) + sales.dept_id.astype(str), drop_first=False).astype(int).values, # Level 7\n",
    "#     pd.get_dummies(sales.store_id.astype(str) + sales.cat_id.astype(str), drop_first=False).astype(int).values, # Level 8\n",
    "#     pd.get_dummies(sales.store_id.astype(str) + sales.dept_id.astype(str), drop_first=False).astype(int).values, # Level 9\n",
    "#     pd.get_dummies(sales.item_id.astype(str), drop_first=False).astype(int).values, # Level 10\n",
    "#     pd.get_dummies(sales.state_id.astype(str) + sales.item_id.astype(str), drop_first=False).astype(int).values, # Level 11\n",
    "#     np.identity(NUM_ITEMS) # Level 12\n",
    "# ].T\n",
    "# WEIGHTS_MAT_BOOL = csr_matrix(WEIGHTS_MAT_BOOL)\n",
    "\n",
    "# def get_s(WEIGHTS_MAT_BOOL, sales):\n",
    "#     WEIGHTS_SALES = WEIGHTS_MAT_BOOL * sales\n",
    "#     weight1 = np.nanmean(\n",
    "#         np.diff(WEIGHTS_SALES, axis=1)**2, axis=1\n",
    "#     )\n",
    "#     return weight1\n",
    "\n",
    "# def get_w(WEIGHTS_MAT_BOOL, total_sales):\n",
    "#     WEIGHTS_TOTAL_SALES = WEIGHTS_MAT_BOOL * total_sales\n",
    "#     weight2 = 12 * WEIGHTS_TOTAL_SALES / np.sum(WEIGHTS_TOTAL_SALES)\n",
    "#     return weight2\n",
    "\n",
    "# DAYS_VALID = [f'd_{i}' for i in range(1886, 1914)]\n",
    "\n",
    "# S = get_s(WEIGHTS_MAT_BOOL, sales[DAYS_VALID].values)\n",
    "# W = get_w(WEIGHTS_MAT_BOOL, total_sales_valid)\n",
    "# del sales; gc.collect()\n",
    "\n",
    "# S_diff = np.full(len(S), 1e-10)\n",
    "# SW = W / np.sqrt(S + S_diff)\n",
    "\n",
    "# def wrmsse(preds, data, s=S, w=W, sw=SW, WEIGHTS_MAT_BOOL=WEIGHTS_MAT_BOOL):\n",
    "#     y_true = data.get_label()\n",
    "#     score = np.sum(\n",
    "#         np.sqrt(\n",
    "#             np.mean(\n",
    "#                 np.square(\n",
    "#                     WEIGHTS_MAT_BOOL * (preds - y_true).reshape(WEIGHTS_MAT_BOOL.shape[1], -1)\n",
    "#                 ), axis=1\n",
    "#             )\n",
    "#         ) * sw\n",
    "#     ) / 12\n",
    "#     return 'wrmsse', score, False\n",
    "\n",
    "# del total_sales_valid, total_sales_eval, df_valid, df_eval; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5:\n",
    "    def __init__(self):\n",
    "        self.TARGET = 'sales'\n",
    "        self.START_TRAIN = 1\n",
    "        self.END_TRAIN = 1913\n",
    "        self.START_VALID = 1914\n",
    "        self.END_VALID = 1941\n",
    "        self.START_TEST = 1942\n",
    "        self.END_TEST = 1969\n",
    "        self.P_HORIZON = 28\n",
    "        self.SEED = 2020\n",
    "        self.N_CORES = psutil.cpu_count()\n",
    "        self.INPUT = '/kaggle/input'\n",
    "        self.ORIGINAL = f'{self.INPUT}/m5-forecasting-accuracy'\n",
    "        self.BASE = f'{self.INPUT}/m5-simple-fe/grid_part_1.pkl'\n",
    "        self.PRICE = f'{self.INPUT}/m5-simple-fe/grid_part_2.pkl'\n",
    "        self.CALENDAR = f'{self.INPUT}/m5-simple-fe/grid_part_3.pkl'\n",
    "        self.LAGS = f'{self.INPUT}/m5-lags-features/lags_df_28.pkl'\n",
    "        self.MEAN_ENC = f'{self.INPUT}/m5-custom-features/mean_encoding_df.pkl'\n",
    "        self.STORES_IDS = list(pd.read_csv(f'{self.ORIGINAL}/sales_train_evaluation.csv')['store_id'].unique())\n",
    "        self.SHIFT_DAY = 28\n",
    "        self.N_LAGS = 15\n",
    "        self.LAGS_SPLIT = [col for col in range(1, self.SHIFT_DAY + self.N_LAGS)]\n",
    "        self.ROLS_SPLIT = [[i, j] for i in [1, 7, 14] for j in [7, 14, 30, 60]]\n",
    "        self.mean_features = [\n",
    "            'enc_cat_id_mean',\n",
    "            'enc_cat_id_std',\n",
    "            'enc_dept_id_mean',\n",
    "            'enc_dept_id_std',\n",
    "            'enc_item_id_mean',\n",
    "            'enc_item_id_std',\n",
    "        ]\n",
    "        self.feature_columns = [\n",
    "            'item_id',\n",
    "            'dept_id',\n",
    "            'cat_id',\n",
    "            'release',\n",
    "            'sell_price',\n",
    "            'price_max',\n",
    "            'price_min',\n",
    "            'price_std',\n",
    "            'price_mean',\n",
    "            'price_norm',\n",
    "            'price_nunique',\n",
    "            'item_nunique',\n",
    "            'price_momentum',\n",
    "            'price_momentum_m',\n",
    "            'price_momentum_y',\n",
    "            'event_name_1',\n",
    "            'event_type_1',\n",
    "            'event_name_2',\n",
    "            'event_type_2',\n",
    "            'snap_CA',\n",
    "            'snap_TX',\n",
    "            'snap_WI',\n",
    "            'tm_d',\n",
    "            'tm_w',\n",
    "            'tm_m',\n",
    "            'tm_y',\n",
    "            'tm_wm',\n",
    "            'tm_dw',\n",
    "            'tm_w_end',\n",
    "            'enc_cat_id_mean',\n",
    "            'enc_cat_id_std',\n",
    "            'enc_dept_id_mean',\n",
    "            'enc_dept_id_std',\n",
    "            'enc_item_id_mean',\n",
    "            'enc_item_id_std',\n",
    "        ]\n",
    "        self.lag_columns = [col for col in pd.read_pickle(self.LAGS).columns if 'lag_' in col or 'rolling_' in col]\n",
    "        \n",
    "        self.pred_series = [\n",
    "            [1, 2],\n",
    "            [3, 4],\n",
    "            [5, 6],\n",
    "            [7, 8],\n",
    "            [9, 10],\n",
    "            [11, 12],\n",
    "            [13, 14],\n",
    "            [15, 16],\n",
    "            [17, 18],\n",
    "            [19, 20],\n",
    "            [21, 22],\n",
    "            [23, 24],\n",
    "            [25, 26],\n",
    "            [27, 28],\n",
    "        ]\n",
    "\n",
    "    def seed_everything(self):\n",
    "        seed = self.SEED\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    def get_data_by_store(self, store_id):\n",
    "        df = pd.read_pickle(self.BASE).query(f'd >= {self.START_TRAIN}').reset_index(drop=True)\n",
    "        \n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.PRICE),\n",
    "            on=['id', 'd']\n",
    "        ).merge(\n",
    "            pd.read_pickle(self.CALENDAR),\n",
    "            on=['id', 'd']\n",
    "        ).query(f'store_id == \"{store_id}\"')\n",
    "        \n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.MEAN_ENC)[['id', 'd'] + self.mean_features],\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "        \n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.LAGS).drop('sales', axis=1),\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "        \n",
    "        # Christmasの日を落とす\n",
    "        df = df.query('event_name_1 != \"Christmas\"').reset_index(drop=True)\n",
    "        return df\n",
    "\n",
    "    def get_data(self):\n",
    "        df = pd.read_pickle(self.BASE).query(f'd >= {self.START_TRAIN}').reset_index(drop=True)\n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.PRICE),\n",
    "            on=['id', 'd']\n",
    "        ).merge(\n",
    "            pd.read_pickle(self.CALENDAR),\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "\n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.MEAN_ENC)[['id', 'd'] + self.mean_features],\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "\n",
    "        df = df.merge(\n",
    "            pd.read_pickle(self.LAGS).drop('sales', axis=1),\n",
    "            how='left',\n",
    "            on=['id', 'd']\n",
    "        )\n",
    "\n",
    "        # Christmasの日を落とす\n",
    "        df = df.query('event_name_1 != \"Christmas\"').reset_index(drop=True)\n",
    "\n",
    "        df = df.sort_values(['d', 'id']).reset_index(drop=True)\n",
    "\n",
    "        return df\n",
    "    def train_test_split(self, df):\n",
    "        df = df.sort_values(['d', 'id'])\n",
    "        train_df = df.query(f'd <= {self.END_VALID}').reset_index(drop=True)\n",
    "        test_df = df.query(f'd >= {self.START_TEST}').reset_index(drop=True)\n",
    "        return train_df, test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Base_Model(object):\n",
    "    def __init__(self, train_df, test_df, features, target, seed, wrmsse, n_splits=3, verbose=True):\n",
    "        self.train_df = train_df\n",
    "        self.test_df = test_df\n",
    "        self.features = features\n",
    "        self.n_splits = n_splits\n",
    "        self.verbose = verbose\n",
    "        self.target = target\n",
    "        self.seed = seed\n",
    "        self.params = self.get_params()\n",
    "        self.wrmsse = wrmsse\n",
    "        self.y_pred, self.score, self.models = self.fit()\n",
    "\n",
    "    def train_model(self, train_set, val_set):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_params(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def convert_x(self, x):\n",
    "        return x\n",
    "\n",
    "    def generate_importance_fig(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def fit(self):\n",
    "        y_pred = np.zeros((len(self.test_df), ))\n",
    "\n",
    "        mlflow.log_param('train_period', self.train_df['d'].max() - self.train_df['d'].min())\n",
    "        mlflow.log_params(self.params)\n",
    "        models = []\n",
    "        \n",
    "        oof_pred = np.array([])\n",
    "        oof_true = np.array([])\n",
    "        for fold in range(1, 1+self.n_splits):\n",
    "            val_idx = self.train_df.query(f'd >= {1942 - 28*fold} and d <= {1941 - 28*(fold-1)}').index\n",
    "            train_idx = self.train_df.query(f'd < {1942 - 28*fold}').index\n",
    "\n",
    "            print(f'FOLD:', fold)\n",
    "            x_train, x_val = self.train_df[self.features].iloc[train_idx], self.train_df[self.features].iloc[val_idx]\n",
    "            y_train, y_val = self.train_df[self.target][train_idx], self.train_df[self.target][val_idx]\n",
    "            train_set, val_set = self.convert_dataset(x_train, y_train, x_val, y_val)\n",
    "            model = self.train_model(train_set, val_set)\n",
    "            models.append(model)\n",
    "            conv_x_val = self.convert_x(x_val)\n",
    "            fold_pred = model.predict(conv_x_val).reshape(self.train_df[self.target][val_idx].shape)\n",
    "            oof_pred = np.concatenate((oof_pred, fold_pred), axis=None)\n",
    "            oof_true = np.concatenate((oof_true, y_val), axis=None)\n",
    "            x_test = self.convert_x(self.test_df[self.features])\n",
    "            y_pred += model.predict(x_test).reshape(y_pred.shape) / self.n_splits\n",
    "            fold_loss_score = self.wrmsse(fold_pred, y_val.values, is_feval=False)\n",
    "            print(f'Partial score of fold {fold} is:', fold_loss_score)\n",
    "            del x_train, x_val, y_train, y_val, train_set, val_set, model, conv_x_val, x_test; gc.collect()\n",
    "\n",
    "        loss_score = self.wrmsse(oof_true, oof_pred, is_feval=False)\n",
    "        print('Our oof wrmsse score is: ', loss_score)\n",
    "        mlflow.log_metric('wrmsse', loss_score)\n",
    "        self.generate_importance_fig(models, 'importance')\n",
    "        return y_pred, loss_score, models\n",
    "\n",
    "class Lgb_Model(Base_Model):\n",
    "    def train_model(self, train_set, val_set):\n",
    "        verbosity = 100 if self.verbose else 0\n",
    "        return lgb.train(\n",
    "            self.params,\n",
    "            train_set,\n",
    "            num_boost_round=5000,\n",
    "            early_stopping_rounds=100,\n",
    "            valid_sets=[train_set, val_set],\n",
    "            verbose_eval=verbosity,\n",
    "            feval=self.wrmsse\n",
    "        )\n",
    "\n",
    "    def convert_dataset(self, x_train, y_train, x_val, y_val):\n",
    "        train_set = lgb.Dataset(x_train, y_train)\n",
    "        val_set = lgb.Dataset(x_val, y_val)\n",
    "        return train_set, val_set\n",
    "\n",
    "    def get_params(self):\n",
    "        params = {\n",
    "            'boosting_type': 'gbdt',\n",
    "            'objective': 'tweedie',\n",
    "            'tweedie_variance_power': 1.1,\n",
    "            'metric': 'custom',\n",
    "            'subsample': 0.5,\n",
    "            'subsample_freq': 1,\n",
    "            'learning_rate': 0.01,\n",
    "            'num_leaves': 2**11-1,\n",
    "            'min_data_in_leaf': 2**12-1,\n",
    "            'feature_fraction': 0.5,\n",
    "            'max_bin': 100,\n",
    "            'boost_from_average': False,\n",
    "            'verbose': -1,\n",
    "        }\n",
    "        params['seed'] = self.seed\n",
    "        return params\n",
    "\n",
    "    def generate_importance_fig(self, models, fig_path):\n",
    "        plt.figure(figsize=(12, 30))\n",
    "        plt.tight_layout()\n",
    "        importance = pd.DataFrame()\n",
    "        importance['column'] = models[0].feature_name()\n",
    "        importance['importance'] = sum([m.feature_importance('gain') for m in models])\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        sns.barplot(x='importance', y='column', data=importance)\n",
    "        plt.savefig(f'{fig_path}_gain.png', bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(f'{fig_path}_gain.png')\n",
    "\n",
    "        plt.figure(figsize=(12, 30))\n",
    "        plt.tight_layout()\n",
    "        importance = pd.DataFrame()\n",
    "        importance['column'] = models[0].feature_name()\n",
    "        importance['importance'] = sum([m.feature_importance('split') for m in models])\n",
    "        importance = importance.sort_values('importance', ascending=False)\n",
    "        sns.barplot(x='importance', y='column', data=importance)\n",
    "        plt.savefig(f'{fig_path}_split.png', bbox_inches=\"tight\")\n",
    "        mlflow.log_artifact(f'{fig_path}_split.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m5 = M5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def convert_submit_data(row):\n",
    "    if row['d'] >= m5.START_VALID and row['d'] <= m5.END_VALID:\n",
    "        row['id'] = row['id'].replace('evaluation', 'validation')\n",
    "        row['d'] = f\"F{row['d'] - m5.END_TRAIN}\"\n",
    "    elif row['d'] >= m5.START_TEST and row['d'] <= m5.END_TEST:\n",
    "        row['id'] = row['id'].replace('validation', 'evaluation')\n",
    "        row['d'] = f\"F{row['d'] - m5.END_VALID}\"\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    return row\n",
    "\n",
    "def get_train_id(threshold, id_frequency):\n",
    "    return id_frequency[id_frequency >= threshold].index.values.astype(str)\n",
    "\n",
    "mlflow.set_experiment('WRMSSE')\n",
    "df = m5.get_data()\n",
    "mlflow.start_run()\n",
    "threshold = 1000\n",
    "mlflow.log_param('threshold', threshold)\n",
    "id_frequency = df['id'].value_counts()\n",
    "train_id = get_train_id(threshold, id_frequency)\n",
    "df = df[df['id'].isin(train_id)].query(f'd >= {m5.END_TEST + 1 - threshold}').reset_index(drop=True)\n",
    "\n",
    "submission = pd.DataFrame(columns=pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sample_submission.csv').columns)\n",
    "\n",
    "## calculate wrmsse----------------------------------------\n",
    "sales = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sales_train_evaluation.csv').drop([f'd_{i}' for i in range(1, 1914)], axis=1)\n",
    "sales = sales[sales['id'].isin(train_id)].reset_index(drop=True)\n",
    "sell_prices = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/sell_prices.csv')\n",
    "calendar = pd.read_csv('/kaggle/input/m5-forecasting-accuracy/calendar.csv')\n",
    "\n",
    "sales = sales.sort_values('id').reset_index(drop=True)\n",
    "data = pd.melt(sales, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'd', value_name = 'sales')\n",
    "data = data.sort_values(['d', 'id'])\n",
    "data = data.merge(calendar[['d', 'wm_yr_wk']], on=['d']).merge(sell_prices, on=['item_id', 'store_id', 'wm_yr_wk'])\n",
    "data = data.drop(['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'wm_yr_wk'], axis=1)\n",
    "data['d'] = data['d'].apply(lambda x: x.split('_')[-1]).astype(int)\n",
    "data['total_sales'] = data['sales'] * data['sell_price']\n",
    "df_valid = data[data['d'] <= 1941].reset_index(drop=True)\n",
    "df_eval = data[data['d'] > 1941].reset_index(drop=True)\n",
    "del data, calendar, sell_prices; gc.collect()\n",
    "\n",
    "total_sales_valid = df_valid.groupby('id').sum()['total_sales'].values\n",
    "total_sales_eval = df_eval.groupby('id').sum()['total_sales'].values\n",
    "\n",
    "NUM_ITEMS = len(sales)\n",
    "WEIGHTS_MAT_BOOL = np.c_[\n",
    "    np.ones([NUM_ITEMS, 1]).astype(int), # Level 1\n",
    "    pd.get_dummies(sales.state_id.astype(str), drop_first=False).astype(int).values, # Level 2\n",
    "    pd.get_dummies(sales.store_id.astype(str), drop_first=False).astype(int).values, # Level 3\n",
    "    pd.get_dummies(sales.cat_id.astype(str), drop_first=False).astype(int).values, # Level 4\n",
    "    pd.get_dummies(sales.dept_id.astype(str), drop_first=False).astype(int).values, # Level 5\n",
    "    pd.get_dummies(sales.state_id.astype(str) + sales.cat_id.astype(str), drop_first=False).astype(int).values, # Level 6\n",
    "    pd.get_dummies(sales.state_id.astype(str) + sales.dept_id.astype(str), drop_first=False).astype(int).values, # Level 7\n",
    "    pd.get_dummies(sales.store_id.astype(str) + sales.cat_id.astype(str), drop_first=False).astype(int).values, # Level 8\n",
    "    pd.get_dummies(sales.store_id.astype(str) + sales.dept_id.astype(str), drop_first=False).astype(int).values, # Level 9\n",
    "    pd.get_dummies(sales.item_id.astype(str), drop_first=False).astype(int).values, # Level 10\n",
    "    pd.get_dummies(sales.state_id.astype(str) + sales.item_id.astype(str), drop_first=False).astype(int).values, # Level 11\n",
    "    np.identity(NUM_ITEMS) # Level 12\n",
    "].T\n",
    "WEIGHTS_MAT_BOOL = csr_matrix(WEIGHTS_MAT_BOOL)\n",
    "print('WEIGHTS_MAT_BOOL', WEIGHTS_MAT_BOOL.shape)\n",
    "\n",
    "def get_s(WEIGHTS_MAT_BOOL, sales):\n",
    "    WEIGHTS_SALES = WEIGHTS_MAT_BOOL * sales\n",
    "    weight1 = np.nanmean(\n",
    "        np.diff(WEIGHTS_SALES, axis=1)**2, axis=1\n",
    "    )\n",
    "    return weight1\n",
    "\n",
    "def get_w(WEIGHTS_MAT_BOOL, total_sales):\n",
    "    WEIGHTS_TOTAL_SALES = WEIGHTS_MAT_BOOL * total_sales\n",
    "    weight2 = 12 * WEIGHTS_TOTAL_SALES / np.sum(WEIGHTS_TOTAL_SALES)\n",
    "    return weight2\n",
    "\n",
    "DAYS_VALID = [f'd_{i}' for i in range(1914, 1941)]\n",
    "\n",
    "S = get_s(WEIGHTS_MAT_BOOL, sales[DAYS_VALID].values)\n",
    "W = get_w(WEIGHTS_MAT_BOOL, total_sales_valid)\n",
    "del sales; gc.collect()\n",
    "\n",
    "S_diff = np.full(len(S), 1e-10)\n",
    "SW = W / np.sqrt(S + S_diff)\n",
    "\n",
    "def wrmsse(preds, data, s=S, w=W, sw=SW, WEIGHTS_MAT_BOOL=WEIGHTS_MAT_BOOL, is_feval=True):\n",
    "    if is_feval:\n",
    "        y_true = data.get_label()\n",
    "    else:\n",
    "        preds = preds.values if type(preds) == pd.core.series.Series else preds\n",
    "        y_true = data\n",
    "        y_true = y_true.values if type(y_true) == pd.core.series.Series else y_true\n",
    "\n",
    "    score = np.sum(\n",
    "        np.sqrt(\n",
    "            np.mean(\n",
    "                np.square(\n",
    "                    WEIGHTS_MAT_BOOL * (preds - y_true).reshape(WEIGHTS_MAT_BOOL.shape[1], -1)\n",
    "                ), axis=1\n",
    "            )\n",
    "        ) * sw\n",
    "    ) / 12\n",
    "    \n",
    "    if is_feval:\n",
    "        return 'wrmsse', score, False\n",
    "    else:\n",
    "        return score\n",
    "\n",
    "del total_sales_valid, total_sales_eval, df_valid, df_eval; gc.collect()\n",
    "## calculate wrmsse----------------------------------------\n",
    "\n",
    "train_df, test_df = m5.train_test_split(df=df)\n",
    "remove_features = []\n",
    "for col in m5.lag_columns:\n",
    "    if 'sales_lag' in col and test_df[col].isnull().sum() > 0:\n",
    "        remove_features.append(col)\n",
    "        print(col)\n",
    "features = m5.feature_columns + [col for col in m5.lag_columns if col not in remove_features]\n",
    "\n",
    "#  trainで欠損値がある行を落とす\n",
    "print('train_df shape is', train_df.shape)\n",
    "for c in tqdm(m5.lag_columns + ['sell_price']):\n",
    "    if c in features and ('sales_lag' in c or 'rolling_mean' in c):\n",
    "        train_df = train_df[train_df[c].notnull()].reset_index(drop=True)\n",
    "        print('drop NaN row...', c, train_df.shape)\n",
    "\n",
    "print(features)\n",
    "\n",
    "lower_limit = train_df[['id', 'd']].groupby('id').min()['d'].max()\n",
    "print('lower_limit', lower_limit)\n",
    "mlflow.log_param('lower_limit', lower_limit)\n",
    "train_df = train_df.query(f'd >= {lower_limit}').reset_index(drop=True)\n",
    "\n",
    "m5.seed_everything()\n",
    "\n",
    "lgb_model = Lgb_Model(\n",
    "    train_df=train_df,\n",
    "    test_df=test_df,\n",
    "    features=features,\n",
    "    target=m5.TARGET,\n",
    "    seed=m5.SEED,\n",
    "    wrmsse=wrmsse,\n",
    ")\n",
    "\n",
    "test_df['pred'] = lgb_model.y_pred\n",
    "test_df = test_df[['id', 'd', 'pred']].apply(convert_submit_data, axis=1)\n",
    "submission = submission.append(pd.pivot(test_df, columns='d', index='id', values='pred').reset_index())\n",
    "submission.to_csv('./submission.csv', index=False)\n",
    "\n",
    "mlflow.log_artifact('./submission.csv')\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
